## Related Work

### Quran Pronunciation Datasets  

We discuss the most important datasets here. [everyayah](everyayah.com) is the largest openly available dataset with 26 complete *Mushafs* segmented and annotated by Ayah by experts like Al Hossary and non-experts such as Fares Abbad. Qdat [osman2021qdat] contains 1509 utterances of single specific Ayahs labeled for three rules: Madd, Ghunna, and Ikhfaa. Although the scale is relatively small, it was widely adopted by the community [10092350], [10092350], and [shaiakhmetov2025evaluation] due to being open-source. The Tarteel v1 dataset [khan2021tarteel] consists of 25K utterances with diacritics and no Tajweed rules. The latter is the Tarteel [tarteelai] private dataset, a massive 9K-hour collection annotated and diacritics  without Tajweed rules. The most recent benchmark is IqraaEval [kheir2025towards], which presents a test set of 2.2 hours from 18 speakers, but uses Modern Standard Arabic (MSA) without Tajweed rules.  

### Quran Pronunciation Models  

To our knowledge, the first work addressing automated pronunciation assessment for the Holy Quran is RDI [sherif2007enhancing], which built a complete system for detecting pronunciation errors. The work does not specify which errors were included or excluded but mentions testing Qalqala, Idgham, and Iqlab rules. It also omits details on Quranic word phoneticization. Subsequent work continued with [abdou2014computer] and [al2018computer], using Deep Neural Networks (DNNs) to replace HMMs and improve the system. Many studies rely on modeling phoneme duration for duration-dependent rules like Madd and Ghunna, e.g., [mohammed2017recognition], [alqadasi2023improving], but use limited datasets and focus on specific verses rather than the entire Quran. Others concentrate on detecting specific rules like Qalqala [10092350] or Ghunna and Madd [shaiakhmetov2025evaluation], [10485145]. However, most efforts except RDI work train on small-scale datasets from specific Quranic chapters.  

At this point, Tarteel [tarteelai] emerges; though lacking Tajweed rules, they built a robust ASR system for diacritized character detection. They developed a crowd-sourced dataset [khan2021tarteel] of 25K utterances (68 hours), later extended via application users to 9K hours of private annotated data. The work most aligned with our vision of detecting all error types (including Tajweed and *Sifat*/articulation attributes) is [putra2012developing]. Although it relies on HMMs and minimal data, it introduces a multi-level detection system: *Makhraj* (phoneme level) and Tajweed rules level.  

### Pretrained Speech Encoders with Self-Supervised Learning (SSL)  

Speech pretraining began early [hinton2006reducing] but was constrained by the sequential nature of Recurrent Neural Networks (RNNs) [hopfield1982neural]. The rise of Transformers [vaswani2017attention] facilitated greater GPU parallelization, enabling large-scale pretraining. BERT [devlin2019bert] using Masked Language Modeling (MLM) introuduce large unsuprvised pretraing which has better results on down stream taks. This soon extended to speech with wav2vec [schneider2019wav2vec] and wav2vec2.0, which added product quantization [baevski2020wav2vec]. Conformer later replaced vanilla Transformers for speech by integrating convolution [gulati2020conformer]. Googleâ€™s Wav2Vec2-BERT [chung2021w2v] then applied MLM to speech. Finally, Facebook extended Wav2Vec2-BERT pretraining [barrault2023seamless] to 4.5M hours (including 110K Arabic hours), ideal for low-resource language fine-tuning.  
